% XeLaTeX can use any Mac OS X font. See the setromanfont command below.
% Input to XeLaTeX is full Unicode, so Unicode characters can be typed directly into the source.

% The next lines tell TeXShop to typeset with xelatex, and to open and save the source with Unicode encoding.

%!TEX program = xelatex
%!TEX TS-program = xelatexmk
%!TEX encoding = UTF-8 Unicode

\documentclass[authoryearcitations]{UoYCSproject}
\author{Andrew Durant}
\title{Developing a platform for common evaluation of self-healing in wireless sensor networks}
%\date{}
\supervisor{James Harbin}
\CSESE
\wordcount{0}

\includes{}

\excludes{}

\abstract{Wireless sensor networks are used in a wide range of applications, and in recent times this is only expanding. Because they are small, low-power devices, and the common network connectivity uses multi-hop routing, network drop-outs and partitioning of devices is a common problem. According to \citet{Tong2009} self-healing by means of mobile nodes still remains a greatly unstudied area. However, more recently developments have been made in diverse ways.

There are two main approaches to network and device management for combating and fixing these issues: the distributed approach and the centralised approach. However the application for a particular WSN determines the effectiveness of a particular algorithm or management paradigm. In certain use cases power usage is less likely to be an important contributing factor but speed of detection and recovery will be more pressing. Whereas in an environmental monitoring situation network nodes are more likely to be physically difficult to get to once deployed, and longevity of battery power is highly important.

The applications distributed approaches are suited to tend to be those where manual intervention is more difficult once a network is deployed. For this reason self-healing and autonomy within the network is of greater importance and this project will focus in on these applications. A variety of algorithms from both approaches will be presented to show this, then particular attention is given to Recovery through Inward Motion (RIM).

From the variety of different implementations of self healing the performance metrics given may not give a fair comparison between them, as algorithms optimised for low power consumption have a very different purpose to those optimised for rapid recovery. This makes it difficult to compare and evaluate self-healing for use within a practical system. Because of this a common platform for comparison between approaches to self-healing is desired. This project aims to develop such a platform and to evaluate it using existing approaches to self-healing, again using RIM as an example. Finally concluding with suggestions for future developments in the field of self-healing wireless sensor networks and further development of the simulation platform.}

\dedication{}

\acknowledgements{}

% More definitions & declarations in project-report.ldf

\pagenumbering{roman}

\begin{document}
\maketitle
\listoffigures
\listoftables
\renewcommand{\lstlistlistingname}{List of Listings}
\lstlistoflistings

\chapter{Introduction}
\label{cha:Introduction}
\pagenumbering{arabic}

Interest in embedded systems and their applications is ever growing in our society. In recent years the cost of developing small systems on chip and running software on embedded computing platforms has plummeted. This has particularly driven the so called `Internet of Things'; filling the world with Internet connected devices to analyse, report on, and control any system in any location from any other remote location. For many of these devices, alongside providing convenience to their users, the goal is collecting data from the environment. This data may be used to provide insight into how we interact with the world, to improve and guide development of infrastructure, or for many other applications.

Wireless Sensor Networks are one specific area of this domain with a wide variety of applications for data gathering within a region of interest. The provide an independent system than can be deployed in any location from public buildings to forests to disaster zones. One consequence of this is that the networks need to be highly adaptable, and are often designed for bespoke situations. If there are changes to the environment they are in the networks need to be able to cope and continue their mission critical objectives. Because wireless sensor networks communicate through multi-hop routing, if links in the communication fail the network becomes dysfunctional and data will be lost from any network partition separate from the sink or root node.

To combat this self-healing processes have been developed for wireless sensor networks. Often the deployment of nodes is done into inaccessible locations, so returning to manually adapt the network is not possible or desirable. One of the key advantages of wireless sensor networks is their low cost; often to the extent of disposability. The largest proportion of this cost is the deployment process, so if increasing the development costs fractionally to include self-healing can provide a longer lasting, more reliable network that needs no further intervention once deployed is negligible compared to the advantages.

\section{Project Aims}

This project aims to investigate self-healing in wireless sensor networks. Research will investigate the current developments in this field leading to a need for a common evaluation tool for approaches to self-healing.

The goal of this project is to produce a wireless sensor network simulation platform as a reference architecture for evaluating self-healing algorithms. The applications of this range from academic investigation during the development of new algorithms or the improvement of existing algorithms, to practical evaluation of candidate systems prior to development and deployment. The platform will be able to simulate and visualise any algorithm implementation on a realistic network.

The project does not aim to produce a novel self-healing algorithm, but during evaluation will suggest what features are critical for general and specific applications, and areas in which it would be useful to focus future research and algorithm development.

\section{Report Structure}

The body of this report consists of 5 chapters that each describe the tasks within the project.

In chapter~\ref{cha:LitReview} the existing literature and work in self-healing wireless sensor networks is reviewed to gain an in-depth understanding of the current state of research, the problems being solved, and how those solutions are evaluated. Each of these topics are categorised in their own section.

Analysis of the problem and the scope of the project is described in chapter~\ref{cha:Problem}; this chapter sets out the objectives that this project is aiming to achieve.

The design and implementation of the project is detailed in chapter~\ref{cha:Design}, describing the decisions made and the reasoning for them, and the specifics of how the system has been built.

Chapter~\ref{cha:Eval} evaluates the systems produced in chapter~\ref{cha:Design} against the metrics discussed from chapter~\ref{cha:LitReview}. The chapter also evaluates the platform itself.

The report is concluded in chapter~\ref{cha:Conclusion} with a review of the results produced and the conclusions that can be made from them. The chapter returns to the objectives defined in chapter~\ref{cha:Problem} and reviews their status. The report is then finished with suggestions for future work in this area, and further development of the platform.

\section{Statement of Ethics}

The intended application of this project is academic. However, there are potential applications for the methods and algorithms discussed for monitoring or affecting humans, animals or the environment. This project does not include such examples and carries no malicious intent.

The project does not involve any human participants. No sensitive information, that could have a negative impact on any person or organisation, has been gathered during the project.

\chapter{Literature Review}
\label{cha:LitReview}

% Introduction

In order to consider self-healing in real-world applications of wireless sensor networks an understanding of the possible applications is required. A survey of approaches to self-healing is then presented along with metrics that can be used to evaluate and compare them. This is followed with a review of evaluation techniques and real-world factors for wireless sensor networks.

\section{WSN applications}

% JAMES %
% Condense the description in 2.1 and potentially move some of this text
% into the general introduction, if you need the word count

The possible applications for wireless sensor networks are diverse and far reaching. The small self-powered devices that can gather data from a wide area, to detect events and communicate information back to a base station either processed or for processing lend themselves to a large number of applications. Each of these applications utilise the WSN in different ways and thereby bring new challenges to the developer of the systems. Due to this the design of the network is highly dependent on the application domain. Just some of the fields in which WSNs have been used in are environmental monitoring, warfare, agriculture, surveillance, medical care, education and micro-surgery.

Habitat monitoring using WSNs has been taking place on an island of ducks where ordinary monitoring techniques cause disturbance to the animals with problems such as increasing mortality and eventual abandonment of the habitat. The use of WSNs that can be deployed prior to seasonal habitation enables constant monitoring over the time without the need to disturb the inhabitants. \citet{Mainwaring2002} detail the requirements of the network and how it has been developed. Longevity of the devices, remote management, and inconspicuous operation are required to ensure the WSN is reliable and the data is able to be collected. The locations used for WSNs are often extreme, particularly for habitat monitoring. Similar requirements to the above are generated by \citet{Biagioni2002} for monitoring endangered plants. Most importantly however for collecting scientific data is ensuring the system is reliable and the user can have confidence in the data.

% JAMES %
% "ensuring the system is reliable" - what measures did they
% take to do this, and what potential failures were assumed? This can
% give insight into the types of failures real applications expect

A very different approach was taken by \citet{Juang2002} in developing ZebraNet. Rather than statically positioned nodes, the devices are attached to the herd of zebra being monitored with collars. The nature of this system is entirely mobile, without even a fixed base station to report to. Because of this the nodes pass data between them when within range forming a distributed collection and storage network. The mobile base station can then be deployed occasionally to collect the dataset only needing to meet a few devices to obtain all of the data gathered by the network. This network is inherently ad-hoc where nodes may or may not come into contact with one another frequently, and there is no fixed or permanent base station to report to. Again longevity is an important requirement as deploying or re-deploying the sensors requires capture of the animals, so the systems must run for at least a year with no intervention.

Environmental monitoring on volcanoes has been investigated by \citet{Werner-Allen2006} where the use of WSNs enables data collection over a wider area for a longer amount of time compared to using traditional equipment. WSN nodes can be deployed easily and left unattended to monitor the volcano area over a period of several weeks or months. However traditional equipment is bulky and heavy, and therefore difficult to deploy, limiting the area of research. The WSN transmitting the collected data back to the base station also removes the need to frequently return to the sensor devices to retrieve data. This is a clear example of the advantages of using wireless sensor networks. However a number of potential issues are raised. The nodes are deployed at the limits of their range to the nearest neighbour: this allows as large an area as possible to be covered, however if a single node fails all other nodes that were dependent upon it will be unreachable. In the scale of this network, where the maximum hop length from the base station is 6 this may seem insignificant, however that is 37.5\% of the network nodes.


\section{Approaches to self-healing in WSNs}



\subsection{Centralised}

The main way centralised approaches are used in WSNs is in the configuration of the network structure to minimise energy use and ensure good coverage of the area of interest \citep{Wang2003,Ding2005,Wang2005,Derr2013}. Centralised evaluation of the network and area of interest prior to deployment ensures the connectivity and coverage requirements of the application are met.
\begin{quote}
Coverage requires that every location in the sensing field is monitored by at least one sensor. Connectivity requires that the network is not partitioned in terms of nodes' communication capability. Note that coverage is affected by sensors' sensitivity, while connectivity is influenced by sensors' communication ranges.
\citep{Wang2005}
\end{quote}
These vary by application for instance, detection and localisation of events require multiple nodes covering smaller areas (a dense coverage) whereas other applications may only need one node within its transmission range (a sparse coverage). By pre-planning to match the deployment to the application needs the network will be much more efficient in power use and accurate in data collection.

Networks with a high degree of connectivity are more resilient to node failures. \Citet{Wang2003} investigate the relationship between coverage and connectivity to produce an integrated solution that is able to maintain both requirements whilst reducing energy consumption. The degree of connectivity is described as $K$-connected, where any $K-1$ nodes may fail without losing network connectivity. So the higher the connectivity ($>K$) the more nodes are able to fail without disrupting the rest of the functioning network.

% JAMES
% 2.2.1 - but does pre-planning risk placing nodes too thinly in the
% event of failure? Trade off could be commented upon
%
% done

Connectivity is important as simply achieving area coverage would result in nodes being placed too thinly in the event of failures, either requiring a large movement in the network or simply not being able to recover. The main goal of pre-planned centralised approaches is to create a deployment that is resilient without further intervention.

Due to the differences in applications a number of application specific methods have been proposed in addition to those previously mentioned \citep{Meguerdichian2001,Meguerdichian2001a,Meguerdichian2003}. All of these provide for different application requirements in different ways. However a single parametrised method that can be applied to many differing applications has been developed by \citet*{Derr2013}. This allows the specification of the number of nodes and the distance between the nodes which defines the desired coverage and connectivity respectively. The algorithm first constructs a triangle based mesh for the given area of interest, for example with NETGEN (Figure~\ref{fig:netgen}); however this creates more nodes than desired. To reduce the number of nodes \citeauthor*{Derr2013} create the mesh simplification algorithm INRCDTS which removes nodes and smooths the mesh until the required number of nodes from the desired network parameters is reached (Figure~\ref{fig:inrcdts}). To ensure that both coverage and connectivity requirements are met the algorithm assumes that the communication range ($R_C$) is greater than $\sqrt{3}$ of the sensing range ($R_S$) i.e.\ $R_C \ge \sqrt{3} R_S$. Based upon this assumption the node's spacing is determined by $\sqrt{3} R_S$ which ensures coverage and therefore connectivity for the network.

\begin{figure}
 \centering
    \begin{subfigure}[t]{.48\textwidth}
        \includegraphics[width=\textwidth]{figures/netgen.png}
        \caption{Mesh generation with NETGEN, maximum edge length of 40; 126 nodes.}
        \label{fig:netgen}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{.48\textwidth}
        \includegraphics[width=\textwidth]{figures/netgen-inrcdts.png}
        \caption{NETGEN mesh simplification with INRCDTS algorithm; 60 nodes.}
        \label{fig:inrcdts}
    \end{subfigure}
 \caption{Mesh generation and simplification.}
 \label{fig:netgen-inrcdts}
\end{figure}

% JAMES
% More detail on citation [8], the parametrised method of Derr and
% Manic, as we discussed last time.
%
% done

Most centralised approaches use a priori information to specify exact deployment in the area of interest. However \citet*{Qu2012} have developed a post deployment particle swarm optimization (PSO) algorithm that is computed centrally to provide optimal coverage and reduce energy consumption. A centralised algorithm is able to understand the entire network, available devices, and area, and therefore produce the optimal distribution of nodes. The nodes are on a mobile platform that is able to move within the area. Reduction of energy consumption is done by dynamically reducing the range of sensing and communication on each device to the minimum needed once the locations have been determined. \citeauthor*{Qu2012} do not address node failures, as they assume all nodes are working throughout the time of use. The distance measurement is also assumed to be for a point-to-point movement with no consideration for difficulties navigating the terrain. The energy considerations take account of the mobility and sensing of the node, the other components power usage is ignored as it is not increased by the algorithm. The centralised calculations for deployment take place on a device with unlimited power. The significance of the algorithm is dependent upon the relative energy consumption of sensing and movement. If sensing requires more energy then the algorithm can provide significant improvements, however when movement has a higher energy cost the saving is negligible.

No consideration is made for the energy cost of transmitting additional messages back to the root node: this is a concern as if all messages must be relayed back, the nodes near the top of the tree will have a significantly larger number of messages to receive and transmit, reducing their lifetime dramatically. This is partly mitigated by the algorithm only running at deployment rather than throughout, however the nodes closest to the root will still be depleted first, and therefore cut off communication in the network. Provision should be made for this, but \citeauthor*{Qu2012} leave it unmentioned.

% JAMES
% Qu and Georgakopolous... is their energy reduction taking into
% account the additional costs of coordinating via the sink
% nodes?
%
% done
%
% concerns over missing information

\subsection{Distributed}

In \citet{Abbasi2007}'s seminal paper DARA (a Distributed Actor Recovery Algorithm) is introduced. This develops a localised scheme to restore a network of mobile nodes when it has become partitioned due to a node failure. The distributed nature of the algorithm avoids involving every single node, only requiring the local nodes to respond. The algorithm selects a node to replace a failed node and coordinates the local nodes to relocate to rebuild the network. As nodes relocate they may cause other partitioning, but this is dealt with in the same way forming a chain reaction until the entire network is reconnected. The self-healing process requires no supervision or centralised control. The use of cascaded movement reduces the overall movement of nodes as instead of entire blocks moving to recover only a few nodes are needed. This contributes to the main goal of minimising the total distance moved by the network nodes.
However, DARA produces particularly bad results when used with cyclic network topologies. When the node movement is cascaded the entire cycle of node will be moved, when only one may need to be for the optimal solution. To combat this a variant is produced that sends flooding messages to check connectivity before moving. This increases message overhead, but can significantly reduce node movement which tends to be more power hungry.

PADRA \citep{Akkaya2008} also aims to restore connectivity to a local area after a node has failed, whilst minimising the total distance travelled, and without external supervision or involvement. However unlike DARA it provides the functionality to detect the cut-vertices that are the cause of partitioning. PADRA determines potential cut vertices in advance by forming a connected dominating set of the whole network. The dominating set is also used for selection of recovery nodes as the dominating node moves to recover the network. Through this PADRA improves significantly upon DARA towards total travel distance of the optimal cascade model.

RIM (Recovery through Inward Motion) \citep{Younis2010} improves upon DARA and PADRA which each need 2-hop knowledge of the network to only requiring 1-hop knowledge on each node. This significantly reduces the network overhead for maintaining the required knowledge of the network topology, however the simplicity means that the distance travelled by the nodes is greater in larger networks for RIM than for DARA or PADRA. Calculating and transmitting a lot of detailed information is often considered too much overhead for low-power, low-complexity systems, particularly if the network can change often or easily. Depending on the application for the WSN the communication overhead to maintain the topology data could be justified over the generally more efficient algorithm.

% Mum
% @TODO : add an example to explain the above point

Another flaw in RIM is that it assumes and requires only one node failure at a time. Whilst this may be the case, failures in WSNs are most commonly battery depletion, which is likely to occur at similar times across the network, or random failure due to environmental conditions, which could happen at any time to one or multiple nodes.

\begin{figure}
 \centering
    \includegraphics[width=0.6\textwidth]{figures/rim.png}
    \caption{(a)--(d) An example for how RIM restoration process; each shaded node moves based on the position of its neighbours, denoted in double-lined circles.}
    \label{fig:rim}
\end{figure}

% JAMES
% DARA, PADRA and RIM... what results do they give and how were they
% evaluated in their papers.

% a bit more explanation of diagrams / annotations

SFRA \citep{Alfadhly2012} is designed specifically to deal with multiple simultaneous failures to combat the multiple failure problem in RIM. Network trees are built from the root node, with local cluster-head nodes, this introduces a fair amount of network overhead compared with RIM. The number of updates needed to send is reduced by waiting for all child node messages before propagating back up the tree.

\begin{figure}
 \centering
    \includegraphics[width=\textwidth]{figures/sfra.png}
    \caption{Detailed example to illustrate how SFRA algorithm restores connectivity after multiple nodes fail.}
    \label{fig:sfra}
\end{figure}

Distributed approaches become much more efficient as the network scales because they are only concerned with the local nodes that are directly reachable. This allows networks built upon distributed algorithms to become much larger, and cover much wider areas as the overhead increase from additional nodes is minimal compared with the exponential increase in complexity from the centralised approaches.

A number of biologically-inspired approaches exist utilising animal and insect coordination and inspired methods of load balancing. \citet{Caliskanelli2014} suggests the application of bee pheromone based load balancing to node distribution. The main thesis applies pheromone signalling (PS) to node redundancy enabling and disabling nodes within a dense area to preserve their life when they are unneeded. By deciding upon a cluster head for an area all other nodes move to a dormant state but coverage is maintained. If an active cluster head fails then a new cluster head will be assigned. Each cluster head emits a pheromone signal that prevents other nodes from becoming the head. PS is then applied to robotic agents to increase service availability though guidance to specific locations. A pheromone signal denotes coverage of a particular area; in load balancing if an area does not have a strong enough signal an node will be activated to provide coverage and emit pheromone to show it. For robot guidance the agents move towards areas that are lacking in pheromone signal as these will be areas in which nodes have depleted their battery or failed.

% JAMES
% pg 12. Ipek's thesis on node distribution... you have phrased it in
% the form of "each node could". I think there is an exact definition of
% in a later chapter of how the mobile robots operated.
%
% done

Another approach based upon the behaviour of ants in a colony has been introduced by \citet{Wang2014}. The system separates mobile nodes from the normal sensor nodes that are static. The network functions as normal on the static nodes. However if there is a need for relocation of a node, due to failure or any other cause, the mobile nodes can pick up the static nodes and travel with them to another location redeploying them there. This reduces the power and cost overheads of adding mobility to all of the nodes. The mobile nodes can carry a larger battery or travel to a charging station when not actively moving nodes.

% JAMES
% How do the bio-inspired protocols contrast critically with each other?
% Comment on notable differences in performance achieved in the papers,
% realism of any dependencies, and the types of failures protected
% against.

% difference in message overhead, control message transmission etc. - assumptions made - failure modes able to recover - time to recover - topologies tested with, random structured etc. - hardware, simulation

A similar approach has been taken by \citet{Xu2015} for mobile nodes with high power capacity to travel between static sensor nodes, however the mobile nodes here are for recharging the static nodes. In most cases the reason for node failure is depletion of the battery: if this is the case, being able to recharge the sensor nodes in place increases the theoretical length of network use indefinitely.

\section{Metrics for evaluating self-healing WSNs}

The most common metric used for evaluating recovery approaches is the distance travelled \citep{Younis2014}. The total distance travelled is ideally minimised as excessive movement for recovery will have high power consumption. Another use of distance is average movement distance per node failure which allows comparison between multi-node and single-node recovery algorithms. Measurement of distance is trivial for simulated environments and therefore gives a good approximation of the power usage.

Another common measurement is the number of messages transmitted with regard to recovery. This shows the overhead produced by the recovery algorithm \citep{Abbasi2007}. As movement and radio use have the highest power usage, measuring and minimising them will produce the system with the lowest power consumption which is the ultimate objective for most WSN implementers.

Measuring the number of nodes that have to be relocated shows the spread of power consumption across the network \citep{Younis2010}. A larger spread means that the time until the first node fails will be increased as the load has been balanced across the nodes. This is obviously countered if nodes are moving unnecessarily.
This metric can then be fed back into the design to decide on the number of mobile nodes required in the network. If the number can be reduced then savings can be made.

% how many nodes need mobility - this could be used to reduce that

As discussed previously, connectivity and coverage are both important to WSNs. The average node degree is a good metric for connectivity as it shows the average number of direct neighbours for each node. Coverage can be measured by the sum area of the sensing radii \citep{Joshi2012}. Higher degrees of connectivity allow for re-routing and improve network reliability without the addition of movement, however it may be desired to reduce the degree of connectivity and thereby the density of the network so that the fewer nodes are needed and costs are lowered.
% coverage with respect to time
% time to recover

% JAMES
% External dependencies such as GPS/localisation should probably be in
% Section 2.3 as a metric
%
% Done, @todo: needs expanding

Most distribution / relocation algorithms require knowledge of the node location. Investigation into this is outside the scope of this paper, however there are many methods of obtaining location from GPS receivers to self-localisation algorithms that are well summarised by \citet*{Hu2004} and \citet*{Mao2007}.

\section{Review of evaluation techniques for WSNs}
\label{sec:simReview}

% JAMES
% 2.4 Evaluation techniques: As we discussed last time, this should be
% about ways to evaluate the metrics chosen, and relative merits of
% simulation vs. hardware test deployment etc.

Developments in wireless sensor networks can only feasibly take place in simulated environments. The cost and time of full or even scaled deployments is far too high to be practical. Reliability is a key issue, and needs to be maximised prior to deployment as difficulties will only be multiplied in a real-world setting and debugging these is a greatly increased challenge. The use of simulation for development and testing prior to deployment is therefore paramount. A large number of simulation environments exist: from generic network modellers to specific WSN frameworks, even simulators that are developed for a specific deployment. \citep{Egea-Lopez2005, Musznicki2012}

% moving towards hardware should the the final goal

Simulation environments must provide accurate models of real-world factors to be reliable and useful for development and testing. Often the more generic simulators provide less of these features, leaving the exercise to the system developer.

% differences in low/high-level simulations

When testing self-healing networks node failures must be induced, to do this a reliability function to control the failure rate of devices is needed. \citet{Lee2004} analyse the failures of nodes, which are predominantly due to energy depletion. Another common failure mode is the irregularity of radio transmissions which are often modelled, at worst, perfectly or at best very simply. \citet{Zhou2004} investigate radio performance and integrity modelling and how to apply real-world data to the simulated environment to achieve greater accuracy. Implementing and using these models becomes a very large challenge. As with any simulation environment more data can always be added to provide accuracy, but at the same time there is a risk of over-fitting a particular situation reducing the accuracy again.

VisualSense \citep{Baldwin2005}, part of the PtolemyII modelling software, provides a robust framework for building WSNs and creating modular system components. This modularity allows the complex modelling of energy consumption, radio transmission and reception, sensing, and data processing to be broken down into smaller components and connected together. \citet{Rosello2009} show how this modular framework can be developed. In this way each component can be developed individually and iteratively improved to provide a better overall simulation model.

One challenge of discrete event simulation as provided in VisualSense is modelling wireless channel collisions. Each event in the system happens instantaneously, so collisions are not obvious unless the timing behaviour of transmissions is explicitly included in the model. As noted by \citeauthor{Rosello2009} the different system states have different power draws and the energy will be depleted by the actions of the system.

% benefits and drawbacks of simulation / real-word factors affecting
% focus on this

% look into further papers from initial applications
% how hardware is used / affects performance

\section{Conclusions}

% JAMES
% Conclusion - include a general chapter conclusion summarising any
% consensus/your view on the literature you've surveyed

% what is seen as most important to combat for self-healing



% what are the key failure modes that are assumed - are they realistic, are there any missed?
% - single node
% - multi node
% - transient

% how are they solved in various forms?
% - is transient targeted, should this therefore be looked at in more detail

The key failure modes seen in the literature are single-node, multi-node and transient failures. Single-node failures are covered extensively, but approaches to them have mostly been developed and improved to combat multi-node failures due to the commonality of environment changes and other factors affecting regions of nodes. DARA, PADRA and RIM target single node failures stating that they cannot provide recovery from multi-node failures but without showing their behaviour for that case. SFRA and the bio-inspired approaches are designed with multi-node failures in mind as their likelihood is relatively high. The nature and timing of failures is relatively unpredictable, power depletion aside. Any node or group of nodes could fail at any one time due to a change in the environment, particularly when the deployment is in an unstable or dangerous area. Handling of multi-node failures therefore is important for network recovery to be achievable. One untested area is that of transient failures and how the algorithms respond to `failed' nodes that return to the network.

% how are they evaluated - is the simulation robust, has there been hardware deployment?
% - all simulation
% - lack of hardware testing?
% - what is the impact of an error in localisation?

Almost all algorithm evaluation in the literature is done by simulation. Whilst simulation can provide realistic data there are often assumptions in the model or perfect representations of, for example, radio transmission. The simulation frameworks are also often bespoke to the algorithm under test, making direct comparisons between the differing approaches tricky. Real world environments are a lot more difficult to work in, and slow down development, but without providing any comparison from the simulations to real world deployments it is hard to evaluate the robustness of the simulation platforms. The impact of other errors in the system are also not shown. Whilst this is treated as out-of-scope for the research being performed, in practice errors in systems do happen. If the goal of each of these algorithms is to provide a network architecture that is able to recover from failures, then testing with failures should take place. Many approaches require accurate location data for each node, which leads to the question: without this, or with a degraded localisation, how would they perform and would they be able to achieve anything?

% why do people use distributed / not use centralised?


% JAMES
% Generally, add some additional diagrams, particularly to illustrate
% the behaviour of your more complex protocols such as SFRA
%
% done

\chapter{Problem Analysis}
\label{cha:Problem}

This chapter specifies what the project aims to achieve, and the requirements and objectives to be reached for the project to be considered a success.

\section{Problem Description}
This project aims to practically and critically evaluate approaches to self-healing in wireless sensor networks. A reference platform for implementing and testing self-healing algorithms will be designed and implemented; this platform will allow comparisons to be made between different approaches within a single environment thereby providing a fair contrast between the algorithms themselves.

An example algorithm will be presented and critical analysis of its design and the evaluation presented by its designers will be conducted. This will provide a structure for obtaining and presenting results from the simulation platform for further algorithm comparison. The scenarios presented will be based upon those found to be common in the literature.

A simulation rather than hardware platform is chosen for a number of reasons. As presented in section~\ref{sec:simReview}: hardware deployments are difficult and costly and extracting the data required for the evaluation metrics becomes a challenge within itself; particularly without affecting the performance of the rest of the system. With simulation all metrics can be gathered without side effects, and with considered design they can accurately emulate real-world environments. Finally the size of deployment is a large factor, simulation can relatively easily run large networks that would be infeasible to create during the evaluation stage of system development. The evaluator may not have access to much if any hardware, whereas anyone can run a simulation.

Experiments on hardware did take place in the preliminary stages of investigation, however the above issues were encountered and the time required to implement a general enough platform for comparison with the simulation platform was too great to be feasible within this project.

% Localisation assumption
The scope of the platform is to accurately simulate communication and power use of the nodes as these are the primary factors affecting longevity of the network and recovery from failure. It is assumed that the nodes have an awareness of their own location, and that could be provided in a number of different ways that are outside the scope of this project.

The project limits the choice of algorithms to those of a distributed nature as the architecture of centralised approaches are very different due to the problems each approach aims to solve. To compare the results of the scenarios produced from the different architectures would be an interesting topic, but is too large for this project.

The platform itself needs to be structured in such a way that it can be easily modified and reused, with the ability to plug-in a variety of self-healing algorithms without need for modification of the overall system. Because of this it should provide interfaces to all the available data of a node that can be made available. The structure of the platform should be such that individual parts can be re-implemented and swapped out to provide a more accurate or a faster running model depending on the needs of the system user.

The implementation should be based upon real hardware, and systems found in the literature, however adaptations may be made to achieve the aims of this project.

\section{Objectives}

This section presents the key and optional objectives for the project. The key objectives should all be achieved to result in a usable research output from the project. And the optional objectives extend the research and provide additional confidence in the platform for use with other algorithms and in other scenarios.

\subsection{Key Objectives}
The project aims to:
\begin{enumerate}
\item develop a simulation platform for wireless sensor networks, providing wireless communication, power management / monitoring, and movement control for the nodes.
\item implement a self-healing algorithm within the platform and run it in a failure scenario.
\item gather metrics from the scenario simulation.
\item use the metrics to evaluate both the platform and the algorithm and to compare the algorithm with its original results.
\end{enumerate}

\subsection{Optional Objectives}
Additionally the project will attempt to:
\begin{enumerate}
\item run further scenarios such as multi-node failure and unreliable data channels.
\item implement further algorithms and compare between those run on the platform.
\end{enumerate}

\chapter{Design and Implementation}
\label{cha:Design}

In this chapter the system design are architecture are described including the decisions made for their development. Firstly the platform architecture is developed drawing upon practices seen in the literature review, then the implementation of self-healing algorithms within the platform is described. Finally scenarios for evaluation are explained to show how the algorithms can be analysed and critiqued.

\section{Platform Architecture}

To build the simulation platform the PtolemyII VisualSense framework \citep{Baldwin2005} was used as it provides a full simulation package with the flexibility and detail to simulate anything that can be modelled. The actor based simulation enables fast prototyping of architectures that can then be developed into optimised Java components. I also had prior experience with PtolemyII so it required less familiarisation time.

Many other simulation platforms are available with different advantages and disadvantages as seen in section~\ref{sec:simReview}. TOSSIM \citep{Levis2003} from TinyOS by \citet{Levis2005} was considered during initial investigation when a simultaneous hardware and simulation platform was being considered as this would enable a single code base for development. However the difficulties on hardware development mentioned previously, alongside the lack of existing structures within TOSSIM resulted in it being rejected. TOSSIM is designed as a low level simulation for TinyOS's execution ``rather than simulating the real world'', and real world simulation is one of the goals of this project.

PtolemyII provides a large number of existing components and an easy to use visualisation so that the state and topology of the network can be observed throughout simulation, this has lead to interesting discoveries which will be discussed later.

The architecture itself is based upon \citet{Rosello2009} using finite state machines (FSMs) for each key component: Communication, Motion, Power Processing, and Sensing. This modular approach ``allows dividing and encapsulating the functionalities included in the node'' which is very important for constructing a general use platform. It also allows individual modules to be worked on and improved individually, they can be changed and swapped out without affecting the rest of the system. VisualSense FSMs can be constructed in layers, where each state may have a state machine or system model of its own. This layering allows highly complex state machines to be implemented much more simply and for different functionality to easily be modelled within different states. This architecture design is based upon real hardware which, whilst different from the hardware within this project, shows the same goal of an accurate hardware simulation model.

Using the actor model entirely can lead in inefficient systems as it constructs a very large number of objects that must intercommunicate to perform even the simplest tasks. PtolemyII alleviates this through the ability to create your own actors purely in Java code, this allows more efficient processing within single components. The initial design of most components within the platform will be using the VisualSense actors, but this can then be developed into single Java actors; providing the desired modularity is retained.

\subsection{Communication}

The communication module interfaces with the outside world. It is linked directly to the wireless outer ports of the node. It controls what messages are received by the processor and assembles messages from the processor into transmissions into the network. The initial implementation of this module is relatively simplistic, rejecting messages that are not sent to it and sending messages either as broadcast or to the parent node. Further development should allow messages to be sent to any destination and more in-depth modelling of reception and transmission patterns for wireless antennae and communication.

The power consumption model is also ideal and therefore unrealistic, as it should have to turn the radio on to send and receive messages, and off to sleep and conserve power. Currently the communication module is able to receive messages at any time, and outputs the standard consumption rate on message transmission and reception. The state machine should be used to signify transmission, reception and sleep state with the relevant power consumption at a constant level over time within each state.

\subsection{Motion}

The motion module provides localisation and movement for the node. As localisation is outside of the scope of this project it is simply assumed that the node is aware of its location by some means that has been abstracted away from. VisualSense provides a location parameter for each actor that can be read and set, so this is used by the module. The current location is periodically updated and checked against a target location which can be set. When the target is set to a location that the node is not currently at the module causes the node to move towards the target over time; this is done by incrementing/decrementing the location parameter of the node towards the target value. By changing location by small amounts over time the speed of the node motion can be controlled by the module instead of simply jumping to the target which is unrealistic.

\subsection{Power}

The power module takes an input from each of the other modules as they work and reduces its stored power level by their consumption over time until the energy level reaches 0. At this point the battery is depleted and it sends out a `Dead' signal. All of the modules have alive states and a single dead state that is the final state. At any point if a `Dead' signal is received the modules should transition immediately to the dead state which ceases all computation of that module. The power level and consumption rates of each component are customisable on each node. They are set in my tests using the Memsic IRIS node data-sheet \citep{Memsic2011}. These consumption rates are shown in table~\ref{tab:powerRates}.

\begin{table}[]
\centering
\begin{tabular}{@{}lr@{}}
\toprule
Node Module     & Power Consumption Rate (mA) \\ \midrule
Communication   & 17                          \\
Sensing         & 16                          \\
Processing data & 8                           \\
Motion          & 2500                        \\ \bottomrule
\end{tabular}
\caption{Power consumption rates of the various node modules}
\label{tab:powerRates}
\end{table}

\subsection{Processing}

The processing module is the heart of the wireless sensor node. It controls and communicates with every other module. To begin with the architecture for this component was designed using actors within VisualSense, this allowed fast prototyping and debugging, however as the full behaviour of the model was developed the complexity of the system increased and actor implementation became unwieldy. Larger parts of connected actors were needing to be duplicated to accommodate all the required functionality, so the actor development was stopped and the processor was moved to Java. This actor model however provided the basis for the architecture of the processor code, and showed what elements were required for the platform and what needed to be implemented by individual algorithms.

\begin{figure}
 \centering
    \includegraphics[width=\textwidth]{figures/processorInternals.png}
    \caption{The partially complete actor model of the processor before re-implementation in Java.}
    \label{fig:processorInternals}
\end{figure}

Moving to Java allowed much more complex behaviour to be defined simply and multiple actions / function reuse could take place without the problems of looping connections that introduce the need for `TimeDelay: 0' actors to act as one way connections. The partially complete actor model can be seen in Figure~\ref{fig:processorInternals}.

As the processor controls the behaviour of the nodes it would need to be re-implemented for each algorithm tested. To minimise this the processor class is designed to be extended for each control algorithm, providing all the other necessary components in the base class for re-use. These can always be overridden as needed according to Java's inheritance model.

The processor has three periodic tasks that are controlled by the clocks in the model. Firstly to create heartbeat messages, these are used to communicate the state of nodes throughout the network. The heartbeat is broadcast to all nodes within the communication range --- the neighbours of the node. The node information is used to construct a tree network architecture with the parent node being the one with the shortest hop count to the root node. Secondly to get data from the sensing module and produce application messages to send back to the root. And thirdly to check the state of the known neighbours.

When messages are received from other nodes application data is forwarded on to the parent, eventually reaching the root, and heartbeat messages are stored in a neighbour data store. As the heartbeat period across the network is known the time between heartbeats from neighbours can be used to check if they are still alive --- the reasoning behind the naming of the heartbeat protocol. This time-out is calculated to be larger than the heartbeat period to reduce the likelihood of transient failures. When the neighbour check is carried out the data store of neighbours is update with the alive state for each neighbour. The processor is then able to read this information to trigger recovery.

\subsection{Sensing}

The sensing module is the simplest: it outputs an environment measurement when triggered, this models the behaviour of reading from a sensor board and the power consumption required to do so and gives data to send in the periodic application messages.

\section{Self-Healing Algorithms}

The self-healing algorithms under test are implemented within the processing module. The base Java class for processing can be extended for the specific functionality required by each algorithm.

Wireless network recovery is initiated when a node discovers a neighbouring node is no longer communicating. This could be for a variety of reasons as discussed in Chapter~\ref{cha:LitReview}. How the recovery is dealt with depends upon the particular algorithm used, and it is the algorithm's responsibility to recognise the reason for non-communication and react accordingly.

To implement a recovery algorithm the periodically called neighbour check function is overridden.

\subsection{Restoring connectivity through Inward Motion (RIM)}

The RIM algorithm was developed by \citet*{Younis2010}. It was chosen for implementation because of its simplicity and performance, the algorithm is clearly presented and seemingly performs well in low density networks with an even distribution of power consumption. This seemed worthwhile to put to the test, particularly as RIM has become a common comparison algorithm since its development.

Using the pseudocode the algorithm was implemented to run when the neighbour check found a non-communicating node. The algorithm then reacts based upon whether the node seems to have failed or just moved away. Various geometric functions are required to compute the new location depending on how many other nodes are moving.

\begin{algorithm}
\begin{algorithmic}[1]
\If{a sensor node $J$ detects a failure of its neighbour $F$}
    \State{\Call{Notify\_Movement}{$J$}}
    \State{$J$ moves towards $F$ until becoming a distance $r/2$ away}
    \If{$J$ is unable to reach any other 1-hop neighbour of $F$}
        \State{$J$ returns to its original location ($F$ is a boundary node) and sends a do-not-move message to its neighbours.}
    \EndIf
\ElsIf{$J$ receives (a) notification message(s) from $F$}
    \If{$I\_Already\_Reconnected$} \label{rim:noChain}
        \State{Done;}
    \EndIf
    \State{$NewPosition \gets$ \Call{Compute\_newPosition}{$J$}}
    \If{$NewPosition \neq CurrentPosition$}
        \State{\Call{Notify\_Movement}{$J$}}
        \State{$J$ moves to $NewPosition$}
    \EndIf
\ElsIf{$J$ receives a ``do-not-move'' message from $F$}
    \State{$J$ does not move towards $F$ (no cascade movement)}
\EndIf
\State{$I\_Already\_Reconnected \gets TRUE$}

\Function{Compute\_newPosition}{$J$}
\State{$NUM\_PriorNBR \gets$ Number of notification messages that $J$ has received with the lowest rank}
\If{$J$ stays connected with all $PriorNBR$ with least rank}
    \State{\Return{$CurrentPosition$}}
\EndIf
\State{$Location\_Senders[] \gets$ New location(s) of (a) sender(s) from which $J$ have received (a) notification message(s)}
\If{$NUM\_PriorNBR = 1$}
    \State{\Return{a point $r$ unit(s) away from $Location\_Sender[0]$ on the direct path to $Location\_Sender[0]$}}
\Else
    \State{Define a circle of radius $r$ around each of $Location\_Sender[]$}
    \If{$NUM\_PriorNBR = 2$}
        \State{\Return{the closest point between two intersection points}}
    \ElsIf{$NUM\_PriorNBR > 3$}
        \State{\Return{the closest point among intersection points which is located inside all other circles}}
    \EndIf
\EndIf
\EndFunction

\Function{Notify\_Movement}{$J$}
\State{Send a message with a rank value increased by 1 to inform all neighbours of $J$ about its motion and its new position.}
\EndFunction

\end{algorithmic}
\caption{Pseudocode for the RIM algorithm}
\label{alg:RIM}
\end{algorithm}

% why did I implement it as I did, what features / changes

\lstinputlisting[language=Java,firstlineandnumber=60,lastline=89,widthgobble=3,float,caption={Java implementation of RIM},label={lst:RIM}]{../sim/actor/NodeProcessorRIM.java}

The implementation of RIM (algorithm~\ref{alg:RIM}) can be seen in listing~\ref{lst:RIM} which is run periodically for each neighbour of a node.

%\subsection{Simultaneous Failures Recovery Approach (SFRA)}

\section{Simulation Scenarios}
\label{sec:scenarios}

To test the algorithms and the platform a variety of scenarios are constructed for the network to run in. These scenarios, their goals, and the reasoning for their use are presented in this section.

\subsection{Single Node Failure}

The single node failure is a common and simple scenario to perform. Whether due to energy depletion, hardware failure, obstruction to transmission, or a software fault an individual node could fail at any time for many reasons. As seen in the literature, this was the first problem scenario self-healing algorithms were designed to solve, therefore it is the base test as to whether or not they are functional and can be classed as self-healing.

The goal of this scenario is to retain / restore connectivity across the network after a single node has failed. For this scenario the simulation was set up with a number of nodes randomly distributed within a 1000x1000 unit area. Once running a node is randomly selected to fail, where it changes to a non-responsive state no longer sending or receiving messages. RIM is designed with this specific scenario in mind and results are presented with varying communication ranges and numbers of deployed nodes. However when re-creating these experiments it was discovered that the random placement combined with lower communication range or a lower deployment number results in a highly partitioned network: a high number of non-connected nodes / clusters that cannot route to the root node.

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rrrr@{}}
\toprule
Deployed nodes & Minimum range & Mean connected nodes & Std. Deviation \\ \midrule
25             & 250           & 24                   & 1.54           \\
50             & 200           & 47.8                 & 3.34           \\
100            & 150           & 99                   & 1              \\
200            & 100           & 196.8                & 3.4            \\ \bottomrule
\end{tabular}%
}
\caption{Minimum ranges for number of randomly deployed nodes in a 1000x1000 unit area.}
\label{tab:nodeRange}
\end{table}

The minimum range deemed suitable for this area for the number of deployed nodes is presented in table~\ref{tab:nodeRange}. As can be seen, these values are much larger parameters than most of the simulations run by \citeauthor*{Younis2010}. It would seem that their random deployments are either non-random enough to create a connected network, or their results for low density networks are incredibly sparse. To test this scenario on my platform the values from table~\ref{tab:nodeRange} were used as the network parameters.

\subsection{Multi Node Failure}

A multi node failure is as likely to occur as a single failure, if not more so. Yet recovery from such a failure is much more complex. As we have seen, until more recent developments in self-healing, very few algorithms were designed to be able to recover from a multi node failure, specifically discounting it as unlikely or too complex to be within scope for their development. The SFRA algorithm completely restructures the topology of the network to combat this issue, leaving behind many previous assumptions and providing a relatively simple solution to the complex problem.

There are a few variations of this scenario. Firstly and most simply is sequential failing nodes, this could be considered as repeated single node failures within a single simulation run. This should not present any particularly new challenges to systems that can recover from single node failures, however the assumptions made in their development may cause problems for recovery.

Secondly simultaneous node failure in distinct regions of the network. Again this should present no further challenge than the single node failure if the nodes a separated by a large enough distance. The challenge appears when they are close enough that a node is required in the recovery for both failed nodes. This challenge was beyond algorithms such as DARA and RIM, and simply not answered for them.

Thirdly regional failure. This is where an entire region of nodes fails simultaneously and is the common consequence from an environmental change, for example, a landslide occurring due to an earthquake that destroys the nodes monitoring that area. In practice it may be unwise to attempt recovery into a lost region if the destructive event is repeated, or has left a region in a state where deployment is not possible, such as lava flow from an erupted volcano.

Again the goal of these scenarios is to recover to a fully connected state with adequate coverage of the area of interest.

\subsection{Further Scenarios}

There are several other scenarios that could be developed for the platform. One particularly useful scenario is using an unreliable communication channel. Doing so would produce a more realistic transmission model and show the resilience to real-world problems in communication for the algorithm under test. All kinds of outside actors can affect radio transmission; interference from other sources or even from within the network itself is a major issue in all forms wireless communication. As discussed in the literature, the radio pattern for transmission and reception vary greatly from the ideal fixed radius circle used in the previous scenarios. Modelling this degradation would be another challenge, but a worthwhile development to improve the simulation platform.

The wireless channel model in PtolemyII provides power loss modelling and probabilistic transmission loss alongside the limited range properties which have been used. And within VisualSense collision detection is possible by providing a message size / transmission time per transmission. Using these to extend the communications module of the nodes could greatly increase the resemblance to real world environments and allow testing in harsh settings during the development of a self-healing algorithm.

\chapter{Evaluation}
\label{cha:Eval}

In this chapter the scenarios developed in section~\ref{sec:scenarios} are evaluated using a variety of metrics to give a quality assessment of the algorithms and to compare them against the original results presented in the literature. This assessment then provides an evaluation of the platform itself and its performance and accuracy can be discussed.

Each scenario is run on each of the different sized networks and repeated to produce an average response for that network set-up. The simulation was run for a fixed amount of time with a random deployment of nodes and random scenario events; this ensured a fair representation between runs. Each run was repeated to give a representative sample of the scenario and check for consistency in the simulation.

\section{Evaluation Metrics}

In the literature review a number of metrics were discussed; here the chosen metrics for the system are described and the reasoning for their choice is explained.

As previously shown, the predominant measure of performance for any wireless sensor network is the energy consumption. One of the main features of wireless sensor networks is their low power demand, so any features introduced that increase this unnecessarily are rejected in development. As with any architecture design the benefits must be weighed against the cost, and energy consumption is a major cost. High energy consumption reduces the lifetime of the network and the amount of data that is able to be collected, and increases the monetary cost of developing and running the network through larger capacity batteries or other changes that must be made due to nodes requiring more power.

Therefore energy consumption is a major concern for developers of self-healing wireless sensor networks. It may be the case that not implementing self-healing would produce a cheaper and longer lasting network, even if nodes need re-deploying manually, due to the cost of the additional power use. Of course the aim in developing such a system is to increase the lifetime of the network and reduce the need for costly re-deployments, but this can only be achieved by keeping the energy consumption down, and potentially reducing it below that of a non-healing network. Both the total consumption and the average consumption can be measured to show if particular nodes are having a particularly high draw, which will often be the case with nodes that have moved compared to those which have not.

The main energy draw in the system is moving the node and any time taken in doing so will likely prevent sensor reading from taking place or data will be lost whilst a node is out of range until it reconnects. Therefore the distance travelled is a useful metric in observing why the energy consumption might be so high. The total distance travelled shows this across the whole network, and the mean distance travelled shows the spread of node travelling around the environment. The goal of this metric is to reduce the amount of motion to only what is required, removing any unnecessary movement. Involving a large number of nodes in recovery (which can be seen using the number of nodes that have moved as a metric) will have different consequences to having few travelling longer distances. Again these differences must be weighed up by system designers and a balance provided between them to achieve an optimal solution.

The degree of connectivity can be measured by the number of neighbours each node is connected to; the mean of these values provides an overall measure for the scenario. This provides a guideline for the density of the network, which affects how many nodes are likely to move when a failure occurs. However a high connectivity should allow re-routing of the network without the need for movement. A good approach to self-healing should combine both routing and movement for the optimal result.

The platform architecture is designed to deliver application messages to the root node. By measuring the number of these transmitted into the network and those received by the root; a percentage of successful message transmissions can be made. This gives a value of robustness to the recovery. Ideally all messages should reach the root, and if nodes fail the network should be restored fully so there is no partitioning from the root. However this can be affected by all kinds of other factors: the unreliable channels of wireless transmission will automatically reduce this, but despite the external factor this metric can still provide useful information about the success of self-healing.

Finally the mean hop count from the root node gives an idea of the topology of the network: if this is high then the topology will be in its worst case of a long individual line with only one route to the root node. With random deployment this is unlikely to occur.

\section{Single Node Failure}

Running the single node failure scenario with 25 nodes produces relatively consistent results (shown in figure~\ref{fig:singlefail_25node}). RIM is able to restore connectivity and reach a stable state in the network within a minute.

\begin{figure}
 \centering
    \includegraphics[width=\textwidth]{figures/singlefail_25node.png}
    \caption{Results from three runs of a 25 node network with RIM.}
    \label{fig:singlefail_25node}
\end{figure}

Even following the guideline parameters discussed previously run 1 had a partitioned network from the random placement and hence has a low percentage of data messages received back at the root node. The travel distance and number of nodes moved stay consistent between runs, as these are dependent upon the density of the network which can be seen by comparing to the results of 50 and 100 node networks. Comparing against the results produced by \citeauthor*{Younis2010} shows that their network must be much more densely packed, as suggested previously, due to the average distance moved being considerably lower. However the trends in the data are consistent with the original results, particularly considering the increase in average travel with high communication ranges.

Energy use is not presented in the original results so cannot be compared. But using the average usage during the simulation the lifetime of the network can be estimated to be around 7 hours with a 2000mAh power source, this is incredibly low however the actual average consumption rate is likely to be much lower than the average presented in this short simulation as the majority of the simulation involved movement. This result shows a known flaw in the simulation platform, and a bad assumption that has been made in development. The communication module does not sleep, and takes a pessimistic 1s use of energy for each transmission and reception. This is gravely exaggerated as it is common for radio use to be in cycles of 250ms active per minute: this arrangement would draw a total of 1.8mAh per day for radio use, compared to the 408mAh currently simulated.

The average neighbour count is 4 which is a fairly high level of connectivity throughout, measuring this over time during the simulation would allow comparison of connectivity before and after recovery. The nature of RIM is to increase connectivity in its recovery process as it brings nodes closer together. The average hop count is also 4 which correlates with the neighbour count given the network size of 25.

Graphical visualisation of the simulation shows some interesting behaviour. Certain arrangements of nodes when a failure occurs can lead to oscillations in location rapidly depleting the energy as the node moves back and forth. This does not seem specific to the implementation of RIM presented here, but is inherent to the algorithm (algorithm~\ref{alg:RIM}). This occurs when a node (\emph{A}) becomes just out of range from two other nodes (\emph{B} and \emph{C}) that have previously moved and set \variable{I\_Already\_Reconnected} true. When the \emph{A} notifies its intention to move \emph{B} and \emph{C} do not respond in chained motion because of lines 8--10 in algorithm~\ref{alg:RIM}, therefore as \emph{A} moves away and loses connection with \emph{B} or \emph{C} it detects them as failed and moves back towards them. When \emph{B} and \emph{C} are either side of \emph{A} this will repeat. As one node comes into range of \emph{A} the other will have become out of range and the cycle continues.

\begin{figure}
 \centering
    \includegraphics[width=\textwidth]{figures/singlefail_50node.png}
    \caption{Results from three runs of a 50 node network with RIM.}
    \label{fig:singlefail_50node}
\end{figure}

This behaviour was occasionally seen in 25 node networks, but was often the case in 50 and 100 node networks, this can be seen in run 3 on figure~\ref{fig:singlefail_50node} where the travel distance is dramatically increased. This run did not actually converge but was still oscillating when the simulation ended. The number of nodes moved has increased due to the increase in the average number of neighbours.

Despite having raised some concerns over the original process and evaluation presented for RIM, the results from my implementation seem consistent with theirs, which would suggest the simulation platform is able to provide accurate data. The data itself may show problems in the algorithm, but the similarity shows accuracy in the implementation and the platform itself.

The 100 node network shows a similar trend, and all runs finished with nodes still oscillating their location. This seems to be a fairly major flaw in the algorithm as it is hugely wasteful of energy, and I suspect that the behaviour would be exhibited on any network with high density or a high node count. However discovering the flaw shows the strength of visual simulation, as it is obvious to any observer, whereas it would be unlikely to be discovered by purely analysing log file output unless it was specifically looked for. It is likely that the statistics presented on RIM would have been automatically generated from the logs without further analysis for oscillations or for network connectivity as previously mentioned.

\section{Multi Node Failure}

\chapter{Conclusions}
\label{cha:Conclusion}

% RIM data seems accurate, but testing methods do not seem overly robust

% Major flaw in simulation platform for energy consumption, but overall a useful platform

% Many improvements could be made, and more scenarios developed.

\bibliography{project-report}

%author = {\c{C}al\i\c{s}kanelli, \.{I}pek},

\end{document}
