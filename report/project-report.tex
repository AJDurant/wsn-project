% XeLaTeX can use any Mac OS X font. See the setromanfont command below.
% Input to XeLaTeX is full Unicode, so Unicode characters can be typed directly into the source.

% The next lines tell TeXShop to typeset with xelatex, and to open and save the source with Unicode encoding.

%!TEX program = xelatex
%!TEX TS-program = xelatexmk
%!TEX encoding = UTF-8 Unicode

\documentclass[authoryearcitations]{UoYCSproject}
\author{Andrew Durant}
\title{Self-Healing in Wireless Sensor Networks}
%\date{}
\supervisor{James Harbin}
\CSESE
\wordcount{0}

\includes{}

\excludes{}

\abstract{Abstract}

\dedication{}

\acknowledgements{}

% More definitions & declarations in project-report.ldf

\begin{document}
\maketitle
\listoffigures
\listoftables
\renewcommand*{\lstlistlistingname}{List of Listings}
\lstlistoflistings

\chapter{Introduction}
\label{cha:Introduction}

\section{Project Aims}

This project aims to investigate self-healing in wireless sensor networks.

\section{Report Structure}

\section{Statement of Ethics}

The intended application of this project is academic. However, there are potential applications for the methods and algorithms discussed for monitoring or affecting humans, animals or the environment. This project does not include such examples and carries no malicious intent.

The project does not involved any human participants. No sensitive information, that could have a negative impact on any person or organisation, has been gathered during the project.

\chapter{Literature Review}
\label{cha:LitReview}

% Introduction

In order to consider self-healing in real-world applications of wireless sensor networks an understanding of the possible applications is required. A survey of approaches to self-healing is then presented along with metrics that can be used to evaluate and compare them. This is followed with a review of evaluation techniques and real-world factors for wireless sensor networks.

\section{WSN applications}

% JAMES %
% Condense the description in 2.1 and potentially move some of this text
% into the general introduction, if you need the wordcount

The possible applications for wireless sensor networks are diverse and far reaching. The small self-powered devices that can gather data from a wide area, to detect events and communicate information back to a base station either processed or for processing lend themselves to a large number of applications. Each of these applications utilise the WSN in different ways and thereby bring new challenges to the developer of the systems. Due to this the design of the network is highly dependent on the application domain. Just some of the fields WSNs have been used in are environmental monitoring, warfare, agriculture, surveillance, medical care, education and micro-surgery.

Habitat monitoring using WSNs has been taking place on an island of ducks where ordinary monitoring techniques cause disturbance to the animals with problems such as increasing mortality and eventual abandonment of the habitat. The use of WSNs that can be deployed prior to seasonal habitation enables constant monitoring over the time without the need to disturb the inhabitants. \citet{Mainwaring2002} detail the requirements of the network and how it has been developed. Longevity of the devices, remote management, and inconspicuous operation are required to ensure the WSN is reliable and the data is able to be collected. The locations used for WSNs are often extreme, particularly for habitat monitoring. Similar requirements to the above are generated by \citet{Biagioni2002} for monitoring endangered plants. Most importantly however for collecting scientific data is ensuring the system is reliable and the user can have confidence in the data.

% JAMES %
% "ensuring the system is reliable" - what measures did they
% take to do this, and what potential failures were assumed? This can
% give insight into the types of failures real applications expect

A very different approach was taken by \citet{Juang2002} in developing ZebraNet. Rather than statically positioned nodes, the devices are attached to the herd of zebra being monitored with collars. The nature of this system is entirely mobile, without even a fixed base station to report to. Because of this the nodes pass data between them when within range forming a distributed collection and storage network. The mobile base station can then be deployed occasionally to collect the dataset only needing to meet a few devices to obtain all of the data gathered by the network. This network is inherently ad-hoc where nodes may or may not come into contact with one another frequently, and there is no fixed or permanent base station to report to. Again longevity is an important requirement as deploying or re-deploying the sensors requires capture of the animals, so the systems must run for at least a year with no intervention.

Environmental monitoring on volcanoes has been investigated by \citet{Werner-Allen2006} where the use of WSNs enables data collection over a wider area for a longer amount of time compared to using traditional equipment. WSN nodes can be deployed easily and left unattended to monitor the volcano area over a period of several weeks or months. Whereas traditional equipment is bulky and heavy, and therefore difficult to deploy limiting the area of research. The WSN transmitting the collected data back to the base station also removes the need to frequently return to the sensor devices to retrieve data. This is a clear example of using the advantages of wireless sensor networks. However a number of potential issues are raised. The nodes are deployed at the limits of their range to the nearest neighbour, this allows as large an area as possible to be covered, however if a single node fails all other nodes that were dependent upon it will be unreachable. In the scale of this network, where the maximum hop length from the base station is 6 this may seem insignificant, however that is 37.5\% of the network nodes.


\section{Approaches to self-healing in WSNs}



\subsection{Centralised}

The main way centralised approaches are used in WSNs is in the configuration of the network structure to minimise energy use and ensure good coverage of the area of interest \citep{Wang2003,Ding2005,Wang2005,Derr2013}. Centralised evaluation of the network and area of interest prior to deployment ensures the connectivity and coverage requirements of the application are met.
\begin{quote}
Coverage requires that every location in the sensing field is monitored by at least one sensor. Connectivity requires that the network is not partitioned in terms of nodes' communication capability. Note that coverage is affected by sensors' sensitivity, while connectivity is influenced by sensors' communication ranges.
\citep{Wang2005}
\end{quote}
These vary by application for instance, detection and localisation of events require multiple nodes covering smaller areas (a dense coverage) whereas other applications may only need one node within its transmission range (a sparse coverage). By pre-planning to match the deployment to the application needs the network will be much more efficient in power use and accurate in data collection.

Networks with a high degree of connectivity are more resilient to node failures. \Citet{Wang2003} investigate the relationship between coverage and connectivity to produce an integrated solution that is able to maintain both requirements whilst reducing energy consumption. The degree of connectivity is described as $K$-connected, where any $K-1$ nodes may fail without loosing network connectivity. So the higher the connectivity ($>K$) the more nodes are able to fail without disrupting the rest of the functioning network.

% JAMES
% 2.2.1 - but does pre-planning risk placing nodes too thinly in the
% event of failure? Trade off could be commented upon

Due to the differences in applications a number of application specific methods have been proposed in addition to those previously mentioned \citep{Meguerdichian2001,Meguerdichian2001a,Meguerdichian2003}. All of these provide for different application requirements in different ways. However a single parametrised method that can be applied to many differing applications has been developed by \citet*{Derr2013}. This allows the specification of the number of nodes and the distance between the nodes which defines the desired coverage and connectivity respectively. The algorithm first constructs a triangle based mesh for the given area of interest, for example with NETGEN (Figure~\ref{fig:netgen}), however this creates more nodes than desired. To reduce the number of nodes \citeauthor*{Derr2013} create the mesh simplification algorithm INRCDTS which removes nodes and smooths the mesh until the required number of nodes from the desired network parameters is reached (Figure~\ref{fig:inrcdts}). To ensure that both coverage and connectivity requirements are met the algorithm assumes that the communication range ($R_C$) is greater than $\sqrt{3}$ of the sensing range ($R_S$) ie. $R_C \ge \sqrt{3} R_S$. Based upon this assumption the nodes spacing is determined by $\sqrt{3} R_S$ which ensures coverage and therefore connectivity for the network.

\begin{figure}
 \centering
    \begin{subfigure}[t]{.48\textwidth}
        \includegraphics[width=\textwidth]{figures/netgen.png}
        \caption{Mesh generation with NETGEN, maximum edge length of 40; 126 nodes.}
        \label{fig:netgen}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{.48\textwidth}
        \includegraphics[width=\textwidth]{figures/netgen-inrcdts.png}
        \caption{NETGEN mesh simplification with INRCDTS algorithm; 60 nodes.}
        \label{fig:inrcdts}
    \end{subfigure}
 \caption{Mesh generation and simplification.}
 \label{fig:netgen-inrcdts}
\end{figure}

% JAMES
% More detail on citation [8], the parametrised method of Derr and
% Manic, as we discussed last time.
%
% done

Most centralised approaches use a priori information to specify exact deployment in the area of interest. However \citet*{Qu2012} have developed a post deployment particle swarm optimization (PSO) algorithm that is computed centrally to provide optimal coverage and reduce energy consumption. A centralised algorithm is able to understand the entire network, available devices, and area, and therefore produce the optimal distribution of nodes. The nodes are on a mobile platform that is able to move within the area. Reduction of energy consumption is done by dynamically reducing the range of sensing and communication on each device to the minimum needed once the locations have been determined. \citeauthor*{Qu2012} do not address node failures, as they assume all node are working throughout the time of use. The distance measurement is also assumed to be for a point-to-point movement with no consideration for difficulties navigating the terrain. The energy considerations take account of the mobility and sensing of the node, the other components power usage is ignored as it is not increased by the algorithm. The centralised calculations for deployment take place on a device with unlimited power. The significance of the algorithm is dependent upon the relative energy consumption of sensing and movement. If sensing requires more energy then the algorithm can provide significant improvements, however when movement has a higher energy cost the saving is negligible.

No consideration is made for the energy cost of transmitting additional messages back to the root node, this is a concern as if all messages must be relayed back, the nodes near the top of the tree will have a significantly larger number of messages to receive and transmit, reducing their lifetime dramatically. This is partly mitigated by the algorithm only running at deployment rather than throughout, however the nodes closest to the root will still be depleted first, and therefore cut off communication in the network. Provision should be made for this, but \citeauthor*{Qu2012} leave it unmentioned.

% JAMES
% Qu and Georgakopolous... is their energy reduction taking into
% account the additional costs of coordinating via the sink
% nodes?
%
% done
%
% concerns over missing information

\subsection{Distributed}

In \citet{Abbasi2007}'s seminal paper DARA (a Distributed Actor Recovery Algorithm) is introduced. Developing a localised scheme to restore a network of mobile nodes when it has become partitioned due to a node failure. The distributed nature of the algorithm avoids involving every single node, only requiring the local nodes to respond. The algorithm selects a node to replace a failed node and coordinates the local nodes to relocate to rebuild the network. As nodes relocate they may cause other partitioning, but this is dealt with in the same way forming a chain reaction until the entire network is reconnected. The self-healing process requires no supervision or centralised control. The use of cascaded movement reduces the overall movement of nodes as instead of entire blocks moving to recover only a few nodes are needed. This contributes to the main goal of minimising the total distance moved by the network nodes.
DARA produces particularly bad results when used with cyclic network topologies. When the node movement is cascaded the entire cycle of node will be moved, when only one may need to be for the optimal solution. To combat this a variant that sends flooding messages to check connectivity before moving is produced. This increases message overhead, but can significantly reduce node movement which tends to be more power hungry.

PADRA \citep{Akkaya2008} also aims to restore connectivity to a local area after a node has failed, whilst minimising the total distance travelled, and without external supervision or involvement. However unlike DARA it provides the functionality to detect the cut-vertices that are the cause of partitioning. PADRA determines potential cut vertices in advance by forming a connected dominating set of the whole network. The dominating set is also used for selection of recovery nodes as the dominating node moves to recover the network. Through this PADRA improves significantly upon DARA towards total travel distance of the optimal cascade model.

RIM (Recovery through Inward Motion) \citep{Younis2010} improves upon DARA and PADRA which each need 2-hop knowledge of the network to only requiring 1-hop knowledge on each node. This significantly reduces the network overhead for maintaining the required knowledge of the network topology, however the simplicity means that the distance travelled by the nodes is greater in larger networks for RIM than for DARA or PADRA. Calculating and transmitting a lot of detailed information is often considered too much overhead for low-power, low-complexity systems, particularly if the network can change often or easily. Depending on the application for the WSN the communication overhead to maintain the topology data could be justified over the generally more efficient algorithm.

Another flaw in RIM is that it assumes and requires only one node failure at a time. Whilst this may be the case, failures in WSNs are most commonly battery depletion, which is likely to occur at similar times across the network, or random failure due to environmental conditions, which could happen at any time to one or multiple nodes.

\begin{figure}
 \centering
    \includegraphics[width=0.6\textwidth]{figures/rim.png}
    \caption{(a)--(d) An example for how RIM restoration process; each shaded node moves based on the position of its neighbours, denoted in double-lined circles.}
    \label{fig:rim}
\end{figure}

% JAMES
% DARA, PADRA and RIM... what results do they give and how were they
% evaluated in their papers.

% a bit more explanation of diagrams / annotations

SFRA \citep{Alfadhly2012} is designed specifically to deal with multiple simultaneous failures to combat the multiple failure problem in RIM. Network trees are built from the root node, with local cluster-head nodes, this introduces a fair amount of network overhead compared with RIM. The number of updates needed to send is reduced by waiting for all child node messages before propagating back up the tree.

\begin{figure}
 \centering
    \includegraphics[width=\textwidth]{figures/sfra.png}
    \caption{Detailed example to illustrate how SFRA algorithm restores connectivity after multiple nodes fail.}
    \label{fig:sfra}
\end{figure}

Distributed approaches become much more efficient as the network scales because they are only concerned with the local nodes that are directly reachable. This allows networks built upon distributed algorithms to become much larger, and cover much wider areas as the overhead increase from additional nodes is minimal compared with the exponential increase in complexity from the centralised approaches.

A number of biologically-inspired approaches exist utilising animal and insect coordination and inspired methods of load balancing. \citet{Caliskanelli2014} suggests the application of bee pheromone based load balancing to node distribution. The main thesis applies pheromone signalling (PS) to node redundancy enabling and disabling nodes within a dense area to preserve their life when they are unneeded. By deciding upon a cluster head for an area all other nodes move to a dormant state but coverage is maintained. If an active cluster head fails then a new cluster head will be assigned. Each cluster head emits a pheromone signal that prevents other nodes from becoming the head. PS is then applied to robotic agents to increase service availability though guidance to specific locations. A pheromone signal denotes coverage of a particular area, in load balancing if an area does not have a strong enough signal an node will be activated to provide coverage and emit pheromone to show it. For robot guidance the agents move towards areas that are lacking in pheromone signal as these will be areas in which nodes have depleted their battery or failed.

% JAMES
% pg 12. Ipek's thesis on node distribution... you have phrased it in
% the form of "each node could". I think there is an exact definition of
% in a later chapter of how the mobile robots operated.
%
% done

Another approach based upon the behaviour of ants in a colony has been introduced by \citet{Wang2014}. The system separates mobile nodes from the normal sensor nodes that are static. The network functions as normal on the static nodes. However if there is a need for relocation of a node, due to failure or any other cause, the mobile nodes can pick up the static nodes and travel with them to another location redeploying them there. This reduces the power and cost overheads of adding mobility to all of the nodes. The mobile nodes can carry a larger battery or travel to a charging station when not actively moving nodes.

% JAMES
% How do the bio-inspired protocols contrast critically with each other?
% Comment on notable differences in performance achieved in the papers,
% realism of any dependencies, and the types of failures protected
% against.

% difference in message overhead, control message transmission etc. - assumptions made - failure modes able to recover - time to recover - topologies tested with, random structured etc. - hardware, simulation

A similar approach has been taken by \citet{Xu2015} for mobile nodes with high power capacity to travel between static sensor nodes, however the mobile nodes here are for recharging the static nodes. In most cases the reason for node failure is depletion of the battery, if this is the case, being able to recharge the sensor nodes in place increases the theoretical length of network use indefinitely.

\section{Metrics for evaluating self-healing WSNs}

The most common metric used for evaluating recovery approaches is the distance travelled \citep{Younis2014}. The total distance travelled is ideally minimised as excessive movement for recovery will have high power consumption. Another use of distance is average movement distance per node failure which allows comparison between multi-node and single-node recovery algorithms. Measurement of distance is trivial for simulated environments and therefore gives a good approximation of the power usage.

Another common measurement is the number of messages transmitted with regard to recovery. This shows the overhead produced by the recovery algorithm \citep{Abbasi2007}. As movement and radio use have the highest power usage, measuring and minimising them will produce the system with the lowest power consumption which is the ultimate objective for most WSN implementers.

Measuring the number of nodes that have to be relocated shows the spread of power consumption across the network \citep{Younis2010}. A larger spread means that the time until the first node fails will be increased as the load has been balanced across the nodes. This is obviously countered if nodes are moving unnecessarily.
This metric can then be fed back into the design to decide on the number of mobile nodes required in the network. If the number can be reduced then savings can be made.

% how many nodes need mobility - this could be used to reduce that

As discussed previously, connectivity and coverage are both important to WSNs. The average node degree is a good metric for connectivity as it shows the average number of direct neighbours for each node. Coverage can be measured by the sum area of the sensing radii \citep{Joshi2012}. Higher degrees of connectivity allow for re-routing and improve network reliability without the addition of movement, however it may be desired to reduce the degree of connectivity and thereby the density of the network so that the fewer nodes are needed and costs are lowered.
% coverage with respect to time
% time to recover

% JAMES
% External dependencies such as GPS/localisation should probably be in
% Section 2.3 as a metric
%
% Done, @todo: needs expanding

Most distribution / relocation algorithms require knowledge of the node location. Investigation into this is outside the scope of this paper, however there are many methods of obtaining location from GPS receivers to self-localisation algorithms that are well summarised by \citet*{Hu2004} and \citet*{Mao2007}.

\section{Review of evaluation techniques for WSNs}

% JAMES
% 2.4 Evaluation techniques: As we discussed last time, this should be
% about ways to evaluate the metrics chosen, and relative merits of
% simulation vs. hardware test deployment etc.

Developments in wireless sensor networks can only feasibly take place in simulated environments. The cost and time of full or even scaled deployments is far too high to be practical. Reliability is a key issue, and needs to be maximised prior to deployment as difficulties will only be multiplied in a real-world setting and debugging these is a greatly increased challenge. The use of simulation for development and testing prior to deployment is therefore paramount. A large number of simulation environments exist: from generic network modellers to specific WSN frameworks, even simulators that are developed for a specific deployment. \citep{Egea-Lopez2005, Musznicki2012}

% moving towards hardware should the the final goal

Simulation environments must provide accurate models of real-world factors to be reliable and useful for development and testing. Often the more generic simulators provide less of these features, leaving the exercise to the system developer.

% differences in low/high-level simulations

When testing self-healing networks node failures must be induced, to do this a reliability function to control the failure rate of devices is needed. \citet{Lee2004} analyse the failures of nodes, which are predominantly due to energy depletion. Another common failure mode is the irregularity of radio transmissions which are often modelled, at worst, perfectly or at best very simply. \citet{Zhou2004} investigate radio performance and integrity modelling and how to apply real-world data to the simulated environment to achieve greater accuracy. Implementing and using these models becomes a very large challenge. As with any simulation environment more data can always be added to provide accuracy, but at the same time there is a risk of over-fitting a particular situation reducing the accuracy again.

VisualSense \citep{Baldwin2005}, part of the PtolemyII modelling software, provides a robust framework for building WSNs and creating modular system components. This modularity allows the complex modelling of energy consumption, radio transmission and reception, sensing, and data processing to be broken down into smaller components and connected together. \citet{Rosello2009} show how this modular framework can be developed. In this way each component can be developed individually and iteratively improved to provide a better overall simulation model.

One challenge of discrete event simulation as provided in VisualSense is modelling wireless channel collisions. Each event in the system happens instantaneously, so collisions are not obvious unless the timing behaviour of transmissions is explicitly included in the model. As noted by \citeauthor{Rosello2009} the different system states have different power draws and the energy will be depleted by the actions of the system.

% benefits and drawbacks of simulation / real-word factors affecting
% focus on this

% look into further papers from initial applications
% how hardware is used / affects performance

\section{Conclusions}

% JAMES
% Conclusion - include a general chapter conclusion summarising any
% consensus/your view on the literature you've surveyed

% what is seen as most important to combat for self-healing


% what are the key failure modes that are assumed - are they realistic, are there any missed?
% - single node
% - multi node
% - transient

% how are they solved in various forms?
% - is transient targeted, should this therefore be looked at in more detail

The key failure modes seen in the literature are single-node, multi-node and transient failures. Single-node failures are covered extensively, but approaches to them have mostly been developed and improved to combat multi-node failures due to the commonality of environment changes and other factors affecting regions of nodes. DARA, PADRA and RIM target single node failures stating that they cannot provide recovery from multi-node failures but without showing their behaviour for that case. SFRA and the bio-inspired approaches are designed with multi-node failures in mind as their likelihood is relatively high. The nature and timing of failures is relatively unpredictable, power depletion aside. Any node or group of nodes could fail at any one time due to a change in the environment, particularly when the deployment is in an unstable or dangerous area. Handling of multi-node failures therefore is important for network recovery to be achievable. One untested area is that of transient failures and how the algorithms respond to `failed' nodes that return to the network.

% how are they evaluated - is the simulation robust, has there been hardware deployment?
% - all simulation
% - lack of hardware testing?
% - what is the impact of an error in localisation?

All most all algorithm evaluation in the literature is done by simulation. Whilst simulation can provide realistic data there are often assumptions in the model or perfect representations of for example radio transmission. The simulation frameworks are also often bespoke to the algorithm under test, making direct comparisons between the differing approaches tricky. Real world environment are a lot more difficult to work in, and slow down development, but without providing any comparison from the simulations to real world deployments it is hard to evaluate the robustness of the simulation platforms. The impact of other errors in the system are also not shown, whilst this is treated as out-of-scope for the research being performed, in practice errors in systems do happen. If the goal of each of these algorithms is to provide a network architecture that is able to recover from failures, then testing with failures should take place. Many approaches require accurate location data for each node, which leads to asking without this, or with a degraded localisation, how would they perform and would they be able to achieve anything?

% why do people use distributed / not use centralised?


% JAMES
% Generally, add some additional diagrams, particularly to illustrate
%the behaviour of your more complex protocols such as SFRA

\chapter{Problem Analysis}



\chapter{Design and Implementation}

\chapter{Evaluation}

\chapter{Conclusions}

Wireless sensor networks are used in a wide range of applications, and in recent times this is only expanding. Due to their nature of being small, low-power devices, and the common network connectivity being multi-hop routing, network drop-outs and partitioning of devices is a common problem that has been tackled in a variety of ways. There are two main approaches to network and device management for combating and fixing these issues, the distributed approach and the centralised approach.

According to \citet{Tong2009} self-healing by means of mobile nodes still remains a greatly unstudied area.

The effectiveness of these two approaches is debated, however the application for a particular WSN determines the effectiveness of a particular algorithm or management paradigm. Certain use cases, for example in industrial equipment monitoring, power usage is less likely to be an important contributing factor but speed of detection and recovery might be more pressing. In an environmental monitoring situation network nodes are more likely to be physically difficult to get to once deployed, and longevity of battery power is highly important.

With these considerations, performance metrics for particular algorithms may not give a fair comparison, as algorithms optimised for low power consumption have a very different purpose to those optimised for rapid recovery. However the approaches and algorithms for communication in WSNs are still very different from traditional networking models because it is common for the network topology and availability to change often and quickly, the storage and network capacity is much lower, and wireless channels are prone to interference and drop-outs.


\bibliography{project-report}

%author = {\c{C}al\i\c{s}kanelli, \.{I}pek},

\end{document}
